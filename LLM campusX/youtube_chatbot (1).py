# -*- coding: utf-8 -*-
"""youtube_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBTPg3LgMJAvA-wRO2ewvuDG3WwWZshy
"""

!pip install -q youtube-transcript-api langchain-community tiktoken python-dotenv

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled

video_id = "Gfr50f6ZBvo"

ytt_api = YouTubeTranscriptApi()
Transcript = ytt_api.fetch(video_id)

Transcript

full_text = " ".join([i.text for i in Transcript])

full_text

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
)
chunks = text_splitter.create_documents([full_text])

len(chunks)

print(chunks[0])

import textwrap

print(textwrap.fill(chunks[0].page_content, width=100))

texts = [chunk.page_content for chunk in chunks]  # get text from each chunk
print(texts)

metadatas = [{"chunk_id": i} for i in range(len(texts))]
ids = [str(i) for i in range(len(texts))]

!pip install -q langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

!pip install -q langchain_chroma

from langchain_chroma import Chroma

vectordb = Chroma(
    collection_name="youtube_collection",
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)

vectordb.add_texts(
    texts=texts,
    metadatas=metadatas,
    ids=ids
)

print(vectordb.get(ids = ['0'], include=['documents', 'embeddings']))

retriever = vectordb.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

retriever

question = "is the topic of nuclear fusion discussed in this video? if yes then what was discussed"
related_docs = retriever.invoke(question)

related_docs

context = "\n".join(doc.page_content for doc in related_docs)

context

from langchain_core.prompts import PromptTemplate

prompts = PromptTemplate(
     template="""
      You are a helpful assistant.
      Answer ONLY from the provided transcript context.
      If the context is insufficient, just say you don't know.

      {context}
      Question: {question}
    """,
    input_variables = ['context', 'question']
)

from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen3-4B-Instruct-2507",
    task="conversational",
    huggingfacehub_api_token="your_hf_token",
)

final_prompt = prompts.invoke({"context": context, "question": question})

final_prompt

chat_model =  ChatHuggingFace(llm=llm)

answer = chat_model.invoke(final_prompt)
print(answer.content)

